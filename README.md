# ğŸ“Š Data Processing and Visualization Application

## ğŸ“Œ Overview

This application is designed to **extract, validate, store, and analyze customer data** from multiple file formats:

* CSV
* JSON
* XML
* Excel
* PDF

It also provides **interactive dashboards** for data visualization.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ parsers/                 â†’ Reads and parses input files
â”‚   â”œâ”€â”€ csv_parser.py
â”‚   â”œâ”€â”€ excel_parser.py
â”‚   â”œâ”€â”€ json_parser.py
â”‚   â”œâ”€â”€ pdf_parser.py
â”‚   â””â”€â”€ xml_parser.py
â”‚
â”œâ”€â”€ validators/              â†’ Validates parsed data using Pydantic models
â”‚   â”œâ”€â”€ customers.py
â”‚   â”œâ”€â”€ retail_transactions.py
â”‚   â”œâ”€â”€ credit_card_transactions.py
â”‚   â”œâ”€â”€ trade_transactions.py
â”‚   â”œâ”€â”€ upi_transactions.py
â”‚   â””â”€â”€ validation_f.py
â”‚
â”œâ”€â”€ frontend/                â†’ UI dashboard (charts, reports)
â”œâ”€â”€ mongo/                   â†’ MongoDB scripts (storage, queries)
â”œâ”€â”€ uploads/                 â†’ Raw uploaded files
â”œâ”€â”€ main.py                  â†’ Main entry point
â””â”€â”€ requirements.txt         â†’ Dependencies
```

---

## âš™ï¸ Installation

### 1. Install Python

Make sure you have **Python 3.10+** installed.

### 2. Create Virtual Environment

```bash
python -m venv venv
```

Activate it:

* **Windows (PowerShell):**

  ```bash
  venv\Scripts\activate
  ```
* **Linux/Mac:**

  ```bash
  source venv/bin/activate
  ```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

**Key dependencies:**

* pandas
* pydantic
* scikit-learn
* pymongo
* matplotlib
* seaborn

---

## ğŸš€ Usage

### 1. Upload Files

Place your input files into the `Datasets 2/` folder.

Examples:

```
Customer_retails_transactions.csv
Customer_Master_Data.json
Customer_UPI_transactions.xlsx
Customer_credit_card_transactions.xml
customer_trade_data_V3_image.pdf
```

### 2. Run Validation

```bash
python -m validators.validation_f
```

**What happens:**

* Parser reads the file
* Validator checks column names, data types, and formats
* Report is generated in `validation_report.json`
* Displays valid vs invalid records

### 3. Store Data in Database

If MongoDB is running, validated data will be stored in collections such as:

* `customers`
* `upi_transactions`
* `credit_card_transactions`

### 4. Run Visualization Dashboard

```bash
python main.py
```

Youâ€™ll see:

* Data summary tiles (files processed, records validated)
* Charts and graphs (transaction volume, trends)
* Error logs for invalid records

---

## ğŸ”„ Typical Workflow

1. Place datasets in `Datasets 2/`
2. Run validation â†’ check errors
3. Store validated data in MongoDB
4. Launch dashboard for visualization & insights

---

## ğŸ“ Notes

* Invalid records are **logged, not discarded**, for manual review.
* MongoDB must be installed and running to use the full pipeline.